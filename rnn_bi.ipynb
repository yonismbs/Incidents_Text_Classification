{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification d'incidents avec un réseau  récurrent bidirectionnel et des *embeddings* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import FloatTensor, LongTensor\n",
    "from typing import List\n",
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_fn = \"./data/incidents_train.json\"\n",
    "validation_json_fn = \"./data/incidents_test.json\"\n",
    "test_json_fn = \"./data/incidents_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction permettant de charger les données\n",
    "def load_incident_dataset(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        incident_list = json.load(fp)\n",
    "    return incident_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'incidents dans train: 2475\n",
      "Nombre d'incidents dans validation: 531\n",
      "Nombre d'incidents dans test: 531\n"
     ]
    }
   ],
   "source": [
    "train_list = load_incident_dataset(train_json_fn)\n",
    "validation_list = load_incident_dataset(validation_json_fn)\n",
    "test_list = load_incident_dataset(test_json_fn)\n",
    "\n",
    "print(\"Nombre d'incidents dans train:\", len(train_list))\n",
    "print(\"Nombre d'incidents dans validation:\", len(validation_list))\n",
    "print(\"Nombre d'incidents dans test:\", len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#On divise nos listes en X(texte) et y(labels) pour chacun des sets\n",
    "\n",
    "X_train = [instance[\"text\"] for instance in train_list]\n",
    "y_train = [instance[\"label\"] for instance in train_list]\n",
    "\n",
    "X_val = [instance[\"text\"] for instance in validation_list]\n",
    "y_val = [instance[\"label\"] for instance in validation_list]\n",
    "\n",
    "X_test = [instance[\"text\"] for instance in test_list]\n",
    "y_test = [instance[\"label\"] for instance in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion de plongements de mots (*embeddings*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from en-core-web-md==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#On telecharge nos embeddings/tokenizer spacy\n",
    "spacy.cli.download(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "embedding_size = nlp.meta['vectors']['width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce bloc de code définit des tokens spéciaux pour le remplissage PAD et les mots inconnus UNK, associant à ces tokens des identifiants (0 et 1) et des vecteurs d'embedding de taille zéro pour chacun, et crée des dictionnaires pour mapper les identifiants aux mots (id2word) et les mots aux identifiants (word2id), ainsi que les identifiants aux embeddings correspondants (id2embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_token = \"<PAD>\"   # mot 0\n",
    "unk_token = \"<UNK>\"    # mot 1\n",
    "zero_vec_embedding = np.zeros(embedding_size, dtype=np.float64)\n",
    "\n",
    "id2word = {}\n",
    "id2word[0] = padding_token \n",
    "id2word[1] = unk_token \n",
    "\n",
    "word2id = {}\n",
    "word2id[padding_token] = 0\n",
    "word2id[unk_token] = 1\n",
    "\n",
    "id2embedding = {}\n",
    "id2embedding[0] = zero_vec_embedding\n",
    "id2embedding[1] = zero_vec_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = 2  # Initialise le compteur d'index à 2, car 0 et 1 sont réservés pour <PAD> et <UNK>\n",
    "vocab = word2id.keys()  # Récupère les mots déjà existants dans le dictionnaire word2id\n",
    "\n",
    "# Parcourt toutes les incidents dans l'ensemble d'entraînement X_train\n",
    "for incident in X_train:\n",
    "    # Tokenise chaque question en mots à l'aide de spacy\n",
    "    for word in nlp(incident):\n",
    "        # Vérifie si le mot n'est pas déjà dans le vocabulaire\n",
    "        if word.text not in vocab:\n",
    "            # Si le mot est nouveau, lui attribue le prochain index disponible\n",
    "            word2id[word.text] = word_index\n",
    "            # Ajoute le mot et son index correspondant au dictionnaire id2word\n",
    "            id2word[word_index] = word.text\n",
    "            # Stocke le vecteur d'embedding du mot dans le dictionnaire id2embedding\n",
    "            id2embedding[word_index] = word.vector\n",
    "            # Incrémente l'index pour le prochain mot unique\n",
    "            word_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction ci dessous sera utilisée pour préparer un lot de données (batch) avant de le passer à notre réseau de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    # Extrait les entrées (x) et les longueurs réelles (x_true_length) de chaque séquence dans le batch\n",
    "    x = [x for x, y in batch]  # Récupère les séquences d'entrée\n",
    "    x_true_length = [len(x) for x, y in batch]  # Calcule la longueur de chaque séquence d'entrée\n",
    "\n",
    "    # Extrait et empile les cibles (y) de chaque élément du batch en un tensor\n",
    "    y = torch.stack([y for x, y in batch], dim=0)\n",
    "\n",
    "    # Rembourre les séquences d'entrée pour qu'elles aient toutes la même longueur et les retourne avec leurs longueurs réelles\n",
    "    return ((pad_sequence(x, batch_first=True), x_true_length), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le bloc de code ci-dessous, une matrice d'embeddings est créée et remplie avec les vecteurs d'embeddings pour chaque mot du vocabulaire, en utilisant un dictionnaire qui mappe les identifiants de mots à leurs embeddings correspondants. Cette matrice est ensuite convertie en un tensor PyTorch, permettant son utilisation dans des réseaux de neurones, et la taille de la couche d'embeddings créée est affichée pour vérification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de la couche d'embeddings: torch.Size([11642, 300])\n"
     ]
    }
   ],
   "source": [
    "# Calcule la taille du vocabulaire basé sur le dictionnaire id2embedding\n",
    "vocab_size = len(id2embedding)\n",
    "# Crée une matrice d'embeddings initialisée à zéro avec la taille du vocabulaire et la taille des embeddings\n",
    "embedding_layer = np.zeros((vocab_size, embedding_size), dtype=np.float32)\n",
    "\n",
    "# Remplit la matrice d'embeddings avec les embeddings correspondants pour chaque ID de token\n",
    "for token_id, embedding in id2embedding.items():\n",
    "    embedding_layer[token_id, :] = embedding  # Affecte l'embedding à la ligne correspondant à l'ID du token\n",
    "\n",
    "# Convertit la matrice d'embeddings de numpy à un tensor PyTorch\n",
    "embedding_layer = torch.from_numpy(embedding_layer)\n",
    "\n",
    "print(\"Taille de la couche d'embeddings:\", embedding_layer.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le réseau comprendra une couche d'embedding en entrée. Cette couche d'embedding sera utilisée pour générer les représentations vectorielles (embeddings) de chaque mot dans une phrase. Les inputs fournis à notre réseau seront des batchs contenant des listes de phrases, où chaque mot est encodé par son identifiant (ID) correspondant. Ces listes seront rembourrées (padded) pour assurer une longueur uniforme au sein d'un batch.\n",
    "\n",
    "En plus des données de la phrase, nous fournirons également à notre fonction une variable x_length. Cette variable est importante pour la fonction pack_padded_sequence de PyTorch, qui permet de gérer efficacement les phrases de longueurs variables dans un batch. pack_padded_sequence crée une représentation compacte de ces batchs, en ignorant les éléments de padding, ce qui améliore l'efficacité du traitement par le réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_state_size, nb_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # Initialise une couche d'embedding à partir d'embeddings pré-entraînés\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings)\n",
    "        # Détermine la taille des embeddings\n",
    "        self.embedding_size = embeddings.size()[1]   \n",
    "        # Initialise un LSTM avec une seule couche, bidirectionnel\n",
    "        self.rnn = nn.LSTM(self.embedding_size, hidden_state_size, 1, batch_first=True, bidirectional=True)\n",
    "        # Initialise une couche de classification qui prend en entrée la taille des états cachés * 2 (pour bidirectionnel) et produit le nombre de classes\n",
    "        self.classification_layer = nn.Linear(hidden_state_size*2, nb_classes)\n",
    "    \n",
    "    def forward(self, x, x_lengths):\n",
    "        # Passe les données d'entrée par la couche d'embedding\n",
    "        x = self.embedding_layer(x)\n",
    "        # Emballe les séquences rembourrées pour le traitement par LSTM\n",
    "        packed_batch = pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        # Passe les données emballées par le LSTM et récupère l'état caché et l'état de cellule\n",
    "        output, (h_n, c_n) = self.rnn(packed_batch)  # Utilise l'état caché de la dernière cellule\n",
    "        # Concatène les états cachés des deux directions du LSTM bidirectionnel\n",
    "        h_n_concatenated = torch.cat((h_n[0], h_n[1]), dim=1)\n",
    "        # Passe l'état caché concaténé par la couche de classification\n",
    "        x = self.classification_layer(h_n_concatenated)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions utilitaires\n",
    "\n",
    "Vous pouvez mettre ici toutes les fonctions qui seront utiles pour les sections suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous définissons une classe Dataset qui permet de construire notre dataset à partir de listes de phrases et de leurs classes correspondantes, en utilisant le dictionnaire word2id défini précédemment. Ce dataset sera ensuite fourni à un DataLoader, qui se chargera de diviser le dataset en batches et d'effectuer le rembourrage nécessaire avec la fonction pad_batch, afin que toutes nos entrées aient la même taille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyDataset(Dataset):\n",
    "    def __init__(self, data , targets, word_to_id, spacy_model):\n",
    "        self.data = data\n",
    "        self.sequences = [None for _ in range(len(data))]\n",
    "        self.targets = targets\n",
    "        self.word2id = word_to_id\n",
    "        self.tokenizer = spacy_model\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Vérifie si la séquence à l'index spécifié a déjà été tokenisée\n",
    "        if self.sequences[index] is None:\n",
    "            # Si non, tokenize la phrase à cet index et stocke le résultat dans self.sequences\n",
    "            self.sequences[index] = self.tokenize(self.data[index]) \n",
    "        return LongTensor(self.sequences[index]), LongTensor([int(self.targets[index])]).squeeze(0)\n",
    "\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        # Utilise le modèle spaCy pour tokeniser la phrase donnée\n",
    "        tokens = [word.text for word in self.tokenizer(sentence)]\n",
    "        # Convertit chaque token en son identifiant correspondant.\n",
    "        # Si le token n'est pas trouvé dans dictionnaire word2id, utilise 1 par défaut, qui est l'identifiant pour <UNK> (mot inconnu).\n",
    "        return [self.word2id.get(token, 1) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'entraînement du modèle, nous utiliserons la bibliothèque Poutyne, qui permet d'automatiser ce processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpacyDataset(X_train, y_train, word2id, nlp)\n",
    "valid_dataset = SpacyDataset(X_val, y_val, word2id, nlp)\n",
    "test_dataset = SpacyDataset(X_test, y_test, word2id, nlp)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dimension = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m 1/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m47.31s \u001b[35mloss:\u001b[94m 1.657397\u001b[35m acc:\u001b[94m 41.616162\u001b[35m fscore_macro:\u001b[94m 0.124085\u001b[35m val_loss:\u001b[94m 1.555383\u001b[35m val_acc:\u001b[94m 44.632768\u001b[35m val_fscore_macro:\u001b[94m 0.168568\u001b[0m\n",
      "Epoch 1: val_acc improved from -inf to 44.63277, saving file to model_t2/_rnnBi/checkpoint_epoch_1.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 2/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m46.78s \u001b[35mloss:\u001b[94m 1.291384\u001b[35m acc:\u001b[94m 54.868687\u001b[35m fscore_macro:\u001b[94m 0.257251\u001b[35m val_loss:\u001b[94m 1.247357\u001b[35m val_acc:\u001b[94m 55.743879\u001b[35m val_fscore_macro:\u001b[94m 0.288286\u001b[0m\n",
      "Epoch 2: val_acc improved from 44.63277 to 55.74388, saving file to model_t2/_rnnBi/checkpoint_epoch_2.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 3/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m48.18s \u001b[35mloss:\u001b[94m 1.000303\u001b[35m acc:\u001b[94m 64.000000\u001b[35m fscore_macro:\u001b[94m 0.369549\u001b[35m val_loss:\u001b[94m 1.139469\u001b[35m val_acc:\u001b[94m 61.016949\u001b[35m val_fscore_macro:\u001b[94m 0.351226\u001b[0m\n",
      "Epoch 3: val_acc improved from 55.74388 to 61.01695, saving file to model_t2/_rnnBi/checkpoint_epoch_3.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 4/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m48.44s \u001b[35mloss:\u001b[94m 0.836603\u001b[35m acc:\u001b[94m 70.585859\u001b[35m fscore_macro:\u001b[94m 0.480489\u001b[35m val_loss:\u001b[94m 1.086270\u001b[35m val_acc:\u001b[94m 64.030132\u001b[35m val_fscore_macro:\u001b[94m 0.444195\u001b[0m\n",
      "Epoch 4: val_acc improved from 61.01695 to 64.03013, saving file to model_t2/_rnnBi/checkpoint_epoch_4.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 5/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m46.36s \u001b[35mloss:\u001b[94m 0.696360\u001b[35m acc:\u001b[94m 75.151515\u001b[35m fscore_macro:\u001b[94m 0.558061\u001b[35m val_loss:\u001b[94m 1.078155\u001b[35m val_acc:\u001b[94m 64.030132\u001b[35m val_fscore_macro:\u001b[94m 0.442641\u001b[0m\n",
      "Epoch 5: val_acc improved from 64.03013 to 64.03013, saving file to model_t2/_rnnBi/checkpoint_epoch_5.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 6/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m48.00s \u001b[35mloss:\u001b[94m 0.597945\u001b[35m acc:\u001b[94m 79.232323\u001b[35m fscore_macro:\u001b[94m 0.629080\u001b[35m val_loss:\u001b[94m 1.136477\u001b[35m val_acc:\u001b[94m 60.451977\u001b[35m val_fscore_macro:\u001b[94m 0.441306\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 7/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m47.24s \u001b[35mloss:\u001b[94m 0.474193\u001b[35m acc:\u001b[94m 84.404040\u001b[35m fscore_macro:\u001b[94m 0.717219\u001b[35m val_loss:\u001b[94m 1.159491\u001b[35m val_acc:\u001b[94m 61.581921\u001b[35m val_fscore_macro:\u001b[94m 0.440251\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 8/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m48.30s \u001b[35mloss:\u001b[94m 0.376178\u001b[35m acc:\u001b[94m 88.606061\u001b[35m fscore_macro:\u001b[94m 0.791025\u001b[35m val_loss:\u001b[94m 1.219391\u001b[35m val_acc:\u001b[94m 62.523541\u001b[35m val_fscore_macro:\u001b[94m 0.434422\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 9/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m48.97s \u001b[35mloss:\u001b[94m 0.294093\u001b[35m acc:\u001b[94m 90.585859\u001b[35m fscore_macro:\u001b[94m 0.830783\u001b[35m val_loss:\u001b[94m 1.294895\u001b[35m val_acc:\u001b[94m 62.335217\u001b[35m val_fscore_macro:\u001b[94m 0.438105\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m10/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m48.42s \u001b[35mloss:\u001b[94m 0.206281\u001b[35m acc:\u001b[94m 93.858586\u001b[35m fscore_macro:\u001b[94m 0.897014\u001b[35m val_loss:\u001b[94m 1.412045\u001b[35m val_acc:\u001b[94m 61.393597\u001b[35m val_fscore_macro:\u001b[94m 0.440997\u001b[0m\n",
      "Restoring data from model_t2/_rnnBi/checkpoint_epoch_5.ckpt\n"
     ]
    }
   ],
   "source": [
    "directory_name = 'model_t2/_rnnBi'\n",
    "\n",
    "model = LSTMClassifier(embedding_layer, hidden_dimension, nb_classes)\n",
    "experiment = Experiment(directory_name, \n",
    "                                model, \n",
    "                                optimizer = \"Adam\", \n",
    "                                task=\"classification\")\n",
    "        \n",
    "logging = experiment.train(train_dataloader, valid_dataloader, epochs=10, disable_tensorboard=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best checkpoint at epoch: 5\n",
      "lr: 0.001, loss: 0.69636, acc: 75.1515, fscore_macro: 0.558061, val_loss: 1.07816, val_acc: 64.0301, val_fscore_macro: 0.442641\n",
      "Loading checkpoint model_t2/_rnnBi/checkpoint_epoch_5.ckpt\n",
      "Running test\n",
      "\u001b[35mTest steps: \u001b[36m34 \u001b[32m1.10s \u001b[35mtest_loss:\u001b[94m 1.078155\u001b[35m test_acc:\u001b[94m 64.030132\u001b[35m test_fscore_macro:\u001b[94m 0.442641\u001b[0m          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 1.0988306250073947,\n",
       " 'test_loss': 1.0781551772507572,\n",
       " 'test_acc': 64.030131826742,\n",
       " 'test_fscore_macro': 0.44264137744903564}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison uni et bi ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre RNN bidirectionnel a une exactitude de 64.03% et un score F macro de 0.4426, des performances inférieures à celles de votre RNN unidirectionnel, qui avait une exactitude de 67.42% et un score F macro de 0.587. Cela suggère que, pour votre ensemble de données spécifique, le RNN unidirectionnel est plus efficace, offrant une meilleure précision et un meilleur équilibre entre précision et rappel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que les RNN bidirectionnels soient souvent préférés pour leur capacité à intégrer des informations contextuelles étendues, il existe des scénarios spécifiques où un RNN unidirectionnel peut surpasser son homologue bidirectionnel en termes de performances, en particulier si les caractéristiques des données et les exigences de la tâche correspondent mieux à une analyse unidirectionnelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le RNN unidirectionnel est meilleur que le bidirectionnel dans notre cas car il évite le surajustement. Les RNN bidirectionnels, bien que puissants, peuvent surajuster, surtout avec des ensembles de données limités ou moins divers. Le RNN unidirectionnel, en traitant les données dans une seule direction, peut mieux généraliser, offrant ainsi de meilleures performances sur des données non vu a l'entrainement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
