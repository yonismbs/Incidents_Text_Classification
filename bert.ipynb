{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification d'incidents avec des modèles *Transformers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import FloatTensor, LongTensor\n",
    "from typing import List\n",
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_json_fn = \"./data/incidents_train.json\"\n",
    "validation_json_fn = \"./data/incidents_test.json\"\n",
    "test_json_fn = \"./data/incidents_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction permettant de charger les données\n",
    "def load_incident_dataset(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        incident_list = json.load(fp)\n",
    "    return incident_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'incidents dans train: 2475\n",
      "Nombre d'incidents dans validation: 531\n",
      "Nombre d'incidents dans test: 531\n"
     ]
    }
   ],
   "source": [
    "train_list = load_incident_dataset(train_json_fn)\n",
    "validation_list = load_incident_dataset(validation_json_fn)\n",
    "test_list = load_incident_dataset(test_json_fn)\n",
    "\n",
    "print(\"Nombre d'incidents dans train:\", len(train_list))\n",
    "print(\"Nombre d'incidents dans validation:\", len(validation_list))\n",
    "print(\"Nombre d'incidents dans test:\", len(test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On normalise les données em mettant tout nos textes en minuscule avec la methode .lower():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [{'text': item['text'].lower(), 'label': item['label']} for item in train_list]\n",
    "validation_list = [{'text': item['text'].lower(), 'label': item['label']} for item in validation_list]\n",
    "test_list = [{'text': item['text'].lower(), 'label': item['label']} for item in test_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On convertit nos labels en int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_int(dataset):\n",
    "    for item in dataset:\n",
    "        item['label'] = int(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_labels_to_int(train_list)\n",
    "convert_labels_to_int(validation_list)\n",
    "convert_labels_to_int(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code organise des données textuelles et leurs étiquettes associées en datasets séparés pour l'entraînement, la validation et le test, en utilisant la bibliothèque datasets de Hugging Face. Il crée ensuite un DatasetDict pour regrouper ces trois datasets, facilitant ainsi leur gestion et leur utilisation dans l'entraînement et l'évaluation des modèles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\"text\": [item['text'] for item in train_list], \n",
    "                                   \"label\": [item['label'] for item in train_list]})\n",
    "\n",
    "validation_list = Dataset.from_dict({\"text\": [item['text'] for item in validation_list], \n",
    "                                  \"label\": [item['label'] for item in validation_list]})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\"text\": [item['text'] for item in test_list], \n",
    "                                  \"label\": [item['label'] for item in test_list]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_list,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On charge notre tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la tokenizeur BERT sur l'ensemble du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2475/2475 [00:02<00:00, 921.58 examples/s]\n",
      "Map: 100%|██████████| 531/531 [00:00<00:00, 968.77 examples/s]\n",
      "Map: 100%|██████████| 531/531 [00:00<00:00, 964.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataCollatorWithPadding s'occupe automatiquement de \"padding\" (ajout de zéros ou d'un autre token spécial) pour que tous les exemples dans un lot aient la même longueur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ligne de code initialise un modèle BERT pré-entraîné pour la classification de séquences, configuré pour gérer neuf catégories distinctes, en utilisant la version \"bert-base-uncased\" de BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code définit une fonction compute_metrics pour évaluer l'exactitude des prédictions d'un modèle de classification en utilisant la métrique 'accuracy' de la bibliothèque Hugging Face datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette section de code configure un entraîneur pour notre modele en spécifiant des paramètres d'entraînement, comme le taux d'apprentissage et la taille des lots, et lie le modèle, les données d'entraînement et de test, ainsi que la méthode de calcul des métriques pour l'entraînement et l'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_BERT\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    use_cpu = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 10%|█         | 155/1550 [23:24<2:45:43,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9650126099586487, 'eval_accuracy': 0.6949152542372882, 'eval_runtime': 72.0784, 'eval_samples_per_second': 7.367, 'eval_steps_per_second': 0.472, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 20%|██        | 310/1550 [48:24<2:14:43,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7241417765617371, 'eval_accuracy': 0.7608286252354048, 'eval_runtime': 79.4789, 'eval_samples_per_second': 6.681, 'eval_steps_per_second': 0.428, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 30%|███       | 465/1550 [1:13:57<1:57:02,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7289449572563171, 'eval_accuracy': 0.7627118644067796, 'eval_runtime': 83.616, 'eval_samples_per_second': 6.35, 'eval_steps_per_second': 0.407, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 500/1550 [1:19:02<2:09:54,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8185, 'learning_rate': 1.3548387096774194e-05, 'epoch': 3.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 40%|████      | 620/1550 [1:39:57<2:24:41,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7495856285095215, 'eval_accuracy': 0.7777777777777778, 'eval_runtime': 83.4006, 'eval_samples_per_second': 6.367, 'eval_steps_per_second': 0.408, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 50%|█████     | 775/1550 [2:06:40<2:00:36,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9325693249702454, 'eval_accuracy': 0.768361581920904, 'eval_runtime': 87.8356, 'eval_samples_per_second': 6.045, 'eval_steps_per_second': 0.387, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 60%|██████    | 930/1550 [2:33:18<1:06:52,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9802774786949158, 'eval_accuracy': 0.7589453860640302, 'eval_runtime': 89.1493, 'eval_samples_per_second': 5.956, 'eval_steps_per_second': 0.381, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 1000/1550 [2:43:21<1:14:33,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2096, 'learning_rate': 7.096774193548388e-06, 'epoch': 6.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 70%|███████   | 1085/1550 [2:56:36<1:12:54,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1070828437805176, 'eval_accuracy': 0.7608286252354048, 'eval_runtime': 85.0249, 'eval_samples_per_second': 6.245, 'eval_steps_per_second': 0.4, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 80%|████████  | 1240/1550 [3:23:06<38:48,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1563774347305298, 'eval_accuracy': 0.7532956685499058, 'eval_runtime': 80.0479, 'eval_samples_per_second': 6.634, 'eval_steps_per_second': 0.425, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 90%|█████████ | 1395/1550 [3:48:25<16:58,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1979649066925049, 'eval_accuracy': 0.7589453860640302, 'eval_runtime': 87.6099, 'eval_samples_per_second': 6.061, 'eval_steps_per_second': 0.388, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1500/1550 [4:06:44<09:32, 11.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0587, 'learning_rate': 6.451612903225807e-07, 'epoch': 9.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 1550/1550 [4:15:09<00:00,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2029919624328613, 'eval_accuracy': 0.7589453860640302, 'eval_runtime': 78.3595, 'eval_samples_per_second': 6.776, 'eval_steps_per_second': 0.434, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1550/1550 [4:15:11<00:00,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 15311.6433, 'train_samples_per_second': 1.616, 'train_steps_per_second': 0.101, 'train_loss': 0.3513666384450851, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1550, training_loss=0.3513666384450851, metrics={'train_runtime': 15311.6433, 'train_samples_per_second': 1.616, 'train_steps_per_second': 0.101, 'train_loss': 0.3513666384450851, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_save_directory = \"./pt_save_pretrainedBERT\"\n",
    "model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [01:10<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.evaluate(tokenized_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7241417765617371,\n",
       " 'eval_accuracy': 0.7608286252354048,\n",
       " 'eval_runtime': 72.2949,\n",
       " 'eval_samples_per_second': 7.345,\n",
       " 'eval_steps_per_second': 0.47,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats affichent une précision de 76% pour notre modèle BERT, marquant la meilleure performance que nous avons réalisée jusqu'à présent. Cette précision remarquable souligne l'efficacité des modèles Transformers encodeurs, dans la réalisation de tâches de classification de textes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
