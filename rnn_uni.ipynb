{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification d'incidents avec un réseau  récurrent unidirectionnel et des *embeddings* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import FloatTensor, LongTensor\n",
    "from typing import List\n",
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_fn = \"./data/incidents_train.json\"\n",
    "validation_json_fn = \"./data/incidents_test.json\"\n",
    "test_json_fn = \"./data/incidents_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction permettant de charger les données\n",
    "def load_incident_dataset(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        incident_list = json.load(fp)\n",
    "    return incident_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'incidents dans train: 2475\n",
      "Nombre d'incidents dans validation: 531\n",
      "Nombre d'incidents dans test: 531\n"
     ]
    }
   ],
   "source": [
    "train_list = load_incident_dataset(train_json_fn)\n",
    "validation_list = load_incident_dataset(validation_json_fn)\n",
    "test_list = load_incident_dataset(test_json_fn)\n",
    "\n",
    "print(\"Nombre d'incidents dans train:\", len(train_list))\n",
    "print(\"Nombre d'incidents dans validation:\", len(validation_list))\n",
    "print(\"Nombre d'incidents dans test:\", len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#On divise nos listes en X(texte) et y(labels) pour chacun des sets\n",
    "\n",
    "X_train = [instance[\"text\"] for instance in train_list]\n",
    "y_train = [instance[\"label\"] for instance in train_list]\n",
    "\n",
    "X_val = [instance[\"text\"] for instance in validation_list]\n",
    "y_val = [instance[\"label\"] for instance in validation_list]\n",
    "\n",
    "X_test = [instance[\"text\"] for instance in test_list]\n",
    "y_test = [instance[\"label\"] for instance in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(set(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion de plongements de mots (*embeddings*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m542.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from en-core-web-md==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/younismbs/anaconda3/envs/myEmv/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge ici notre model spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "embedding_size = nlp.meta['vectors']['width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce bloc de code définit des tokens spéciaux pour le remplissage PAD et les mots inconnus UNK, associant à ces tokens des identifiants (0 et 1) et des vecteurs d'embedding de taille zéro pour chacun, et crée des dictionnaires pour mapper les identifiants aux mots (id2word) et les mots aux identifiants (word2id), ainsi que les identifiants aux embeddings correspondants (id2embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_token = \"<PAD>\"   # mot 0\n",
    "unk_token = \"<UNK>\"    # mot 1\n",
    "zero_vec_embedding = np.zeros(embedding_size, dtype=np.float64)\n",
    "\n",
    "id2word = {}\n",
    "id2word[0] = padding_token \n",
    "id2word[1] = unk_token \n",
    "\n",
    "word2id = {}\n",
    "word2id[padding_token] = 0\n",
    "word2id[unk_token] = 1\n",
    "\n",
    "id2embedding = {}\n",
    "id2embedding[0] = zero_vec_embedding\n",
    "id2embedding[1] = zero_vec_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = 2  # Initialise le compteur d'index à 2, car 0 et 1 sont réservés pour <PAD> et <UNK>\n",
    "vocab = word2id.keys()  # Récupère les mots déjà existants dans le dictionnaire word2id\n",
    "\n",
    "# Parcourt toutes les incidents dans l'ensemble d'entraînement X_train\n",
    "for incident in X_train:\n",
    "    # Tokenise chaque question en mots à l'aide de spacy\n",
    "    for word in nlp(incident):\n",
    "        # Vérifie si le mot n'est pas déjà dans le vocabulaire\n",
    "        if word.text not in vocab:\n",
    "            # Si le mot est nouveau, lui attribue le prochain index disponible\n",
    "            word2id[word.text] = word_index\n",
    "            # Ajoute le mot et son index correspondant au dictionnaire id2word\n",
    "            id2word[word_index] = word.text\n",
    "            # Stocke le vecteur d'embedding du mot dans le dictionnaire id2embedding\n",
    "            id2embedding[word_index] = word.vector\n",
    "            # Incrémente l'index pour le prochain mot unique\n",
    "            word_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La fonction ci-dessous sera utilisée par nos DataLoaders pour effectuer le rembourrage des mini-batchs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    # Extrait les entrées (x) et les longueurs réelles (x_true_length) de chaque séquence dans le batch\n",
    "    x = [x for x, y in batch]  # Récupère les séquences d'entrée\n",
    "    x_true_length = [len(x) for x, y in batch]  # Calcule la longueur de chaque séquence d'entrée\n",
    "\n",
    "    # Extrait et empile les cibles (y) de chaque élément du batch en un tensor\n",
    "    y = torch.stack([y for x, y in batch], dim=0)\n",
    "\n",
    "    # Rembourre les séquences d'entrée pour qu'elles aient toutes la même longueur et les retourne avec leurs longueurs réelles\n",
    "    return ((pad_sequence(x, batch_first=True), x_true_length), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le réseau comprendra une couche d'embedding en entrée. Cette couche d'embedding sera utilisée pour générer les représentations vectorielles (embeddings) de chaque mot dans une phrase. Les inputs fournis à notre réseau seront des batchs contenant des listes de phrases, où chaque mot est encodé par son identifiant (ID) correspondant. Ces listes seront rembourrées (padded) pour assurer une longueur uniforme au sein d'un batch.\n",
    "\n",
    "En plus des données de la phrase, nous fournirons également à notre fonction une variable x_length. Cette variable est importante pour la fonction pack_padded_sequence de PyTorch, qui permet de gérer efficacement les phrases de longueurs variables dans un batch. pack_padded_sequence crée une représentation compacte de ces batchs, en ignorant les éléments de padding, ce qui améliore l'efficacité du traitement par le réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_state_size, nb_classes) :\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # Initialise une couche d'embedding à partir d'embeddings pré-entraînés\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings)\n",
    "        # Détermine la taille des embeddings\n",
    "        self.embedding_size = embeddings.size()[1] \n",
    "        # Initialise un LSTM avec une seule couche, unidirectionnel  \n",
    "        self.rnn = nn.LSTM(self.embedding_size, hidden_state_size, 1, batch_first=True)\n",
    "        self.classification_layer = nn.Linear(hidden_state_size, nb_classes)\n",
    "    \n",
    "    def forward(self, x, x_lengths):\n",
    "        # Passe les données d'entrée par la couche d'embedding\n",
    "        x = self.embedding_layer(x)\n",
    "        # Emballe les séquences rembourrées pour le traitement par LSTM\n",
    "        packed_batch = pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        output, (h_n, c_n) = self.rnn(packed_batch)  # On utilise le hidden state de la dernière cellule\n",
    "        x = h_n.squeeze()  # Le LSTM a une seule couche, on retire cette dimension\n",
    "        x = self.classification_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le bloc de code ci-dessous, une matrice d'embeddings est créée et remplie avec les vecteurs d'embeddings pour chaque mot du vocabulaire, en utilisant un dictionnaire qui mappe les identifiants de mots à leurs embeddings correspondants. Cette matrice est ensuite convertie en un tensor PyTorch, permettant son utilisation dans des réseaux de neurones, et la taille de la couche d'embeddings créée est affichée pour vérification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de la couche d'embeddings: torch.Size([11642, 300])\n"
     ]
    }
   ],
   "source": [
    "# Calcule la taille du vocabulaire basé sur le dictionnaire id2embedding\n",
    "vocab_size = len(id2embedding)\n",
    "# Crée une matrice d'embeddings initialisée à zéro avec la taille du vocabulaire et la taille des embeddings\n",
    "embedding_layer = np.zeros((vocab_size, embedding_size), dtype=np.float32)\n",
    "\n",
    "# Remplit la matrice d'embeddings avec les embeddings correspondants pour chaque ID de token\n",
    "for token_id, embedding in id2embedding.items():\n",
    "    embedding_layer[token_id, :] = embedding  # Affecte l'embedding à la ligne correspondant à l'ID du token\n",
    "\n",
    "# Convertit la matrice d'embeddings de numpy à un tensor PyTorch\n",
    "embedding_layer = torch.from_numpy(embedding_layer)\n",
    "\n",
    "print(\"Taille de la couche d'embeddings:\", embedding_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions utilitaires\n",
    "\n",
    "Vous pouvez mettre ici toutes les fonctions qui seront utiles pour les sections suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous définissons une classe Dataset qui permet de construire notre dataset à partir de listes de phrases et de leurs classes correspondantes, en utilisant le dictionnaire word2id défini précédemment. Ce dataset sera ensuite fourni à un DataLoader, qui se chargera de diviser le dataset en batches et d'effectuer le rembourrage nécessaire avec la fonction pad_batch, afin que toutes nos entrées aient la même taille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyDataset(Dataset):\n",
    "    def __init__(self, data , targets, word_to_id, spacy_model):\n",
    "        self.data = data\n",
    "        self.sequences = [None for _ in range(len(data))]\n",
    "        self.targets = targets\n",
    "        self.word2id = word_to_id\n",
    "        self.tokenizer = spacy_model\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Vérifie si la séquence à l'index spécifié a déjà été tokenisée\n",
    "        if self.sequences[index] is None:\n",
    "            # Si non, tokenize la phrase à cet index et stocke le résultat dans self.sequences\n",
    "            self.sequences[index] = self.tokenize(self.data[index]) \n",
    "        return LongTensor(self.sequences[index]), LongTensor([int(self.targets[index])]).squeeze(0)\n",
    "\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        # Utilise le modèle spaCy pour tokeniser la phrase donnée\n",
    "        tokens = [word.text for word in self.tokenizer(sentence)]\n",
    "        # Convertit chaque token en son identifiant correspondant.\n",
    "        # Si le token n'est pas trouvé dans dictionnaire word2id, utilise 1 par défaut, qui est l'identifiant pour <UNK> (mot inconnu).\n",
    "        return [self.word2id.get(token, 1) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'entraînement du modèle, nous utiliserons la bibliothèque Poutyne, qui permet d'automatiser ce processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpacyDataset(X_train, y_train, word2id, nlp)\n",
    "valid_dataset = SpacyDataset(X_val, y_val, word2id, nlp)\n",
    "test_dataset = SpacyDataset(X_test, y_test, word2id, nlp)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le choix d'une taille de couche cachée de 150 pour le RNN est principalement justifié par la performance empirique observée lors des tests. Après avoir expérimenté avec différentes tailles de couches cachées, il a été constaté que 150 unités offraient un équilibre optimal en termes de précision, de perte de test, et de temps de calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dimension = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m 1/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m2m36.72s \u001b[35mloss:\u001b[94m 1.620720\u001b[35m acc:\u001b[94m 44.080808\u001b[35m fscore_macro:\u001b[94m 0.140311\u001b[35m val_loss:\u001b[94m 1.383634\u001b[35m val_acc:\u001b[94m 50.659134\u001b[35m val_fscore_macro:\u001b[94m 0.197170\u001b[0m\n",
      "Epoch 1: val_acc improved from -inf to 50.65913, saving file to model_t2/_rnnUni/checkpoint_epoch_1.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 2/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m40.97s \u001b[35mloss:\u001b[94m 1.151229\u001b[35m acc:\u001b[94m 60.363636\u001b[35m fscore_macro:\u001b[94m 0.304882\u001b[35m val_loss:\u001b[94m 1.120377\u001b[35m val_acc:\u001b[94m 60.828625\u001b[35m val_fscore_macro:\u001b[94m 0.337122\u001b[0m\n",
      "Epoch 2: val_acc improved from 50.65913 to 60.82863, saving file to model_t2/_rnnUni/checkpoint_epoch_2.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 3/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m40.38s \u001b[35mloss:\u001b[94m 0.899050\u001b[35m acc:\u001b[94m 68.525253\u001b[35m fscore_macro:\u001b[94m 0.423974\u001b[35m val_loss:\u001b[94m 1.029107\u001b[35m val_acc:\u001b[94m 63.465160\u001b[35m val_fscore_macro:\u001b[94m 0.419636\u001b[0m\n",
      "Epoch 3: val_acc improved from 60.82863 to 63.46516, saving file to model_t2/_rnnUni/checkpoint_epoch_3.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 4/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m43.04s \u001b[35mloss:\u001b[94m 0.743714\u001b[35m acc:\u001b[94m 74.303030\u001b[35m fscore_macro:\u001b[94m 0.527109\u001b[35m val_loss:\u001b[94m 1.019130\u001b[35m val_acc:\u001b[94m 65.536723\u001b[35m val_fscore_macro:\u001b[94m 0.485139\u001b[0m\n",
      "Epoch 4: val_acc improved from 63.46516 to 65.53672, saving file to model_t2/_rnnUni/checkpoint_epoch_4.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 5/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m42.47s \u001b[35mloss:\u001b[94m 0.607008\u001b[35m acc:\u001b[94m 78.787879\u001b[35m fscore_macro:\u001b[94m 0.631114\u001b[35m val_loss:\u001b[94m 1.002641\u001b[35m val_acc:\u001b[94m 67.419962\u001b[35m val_fscore_macro:\u001b[94m 0.587087\u001b[0m\n",
      "Epoch 5: val_acc improved from 65.53672 to 67.41996, saving file to model_t2/_rnnUni/checkpoint_epoch_5.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 6/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m42.62s \u001b[35mloss:\u001b[94m 0.497565\u001b[35m acc:\u001b[94m 82.747475\u001b[35m fscore_macro:\u001b[94m 0.693617\u001b[35m val_loss:\u001b[94m 1.044713\u001b[35m val_acc:\u001b[94m 66.666667\u001b[35m val_fscore_macro:\u001b[94m 0.584154\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 7/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m38.61s \u001b[35mloss:\u001b[94m 0.369988\u001b[35m acc:\u001b[94m 87.757576\u001b[35m fscore_macro:\u001b[94m 0.775336\u001b[35m val_loss:\u001b[94m 1.122283\u001b[35m val_acc:\u001b[94m 65.725047\u001b[35m val_fscore_macro:\u001b[94m 0.591083\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 8/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m38.45s \u001b[35mloss:\u001b[94m 0.280470\u001b[35m acc:\u001b[94m 91.434343\u001b[35m fscore_macro:\u001b[94m 0.861930\u001b[35m val_loss:\u001b[94m 1.205425\u001b[35m val_acc:\u001b[94m 67.419962\u001b[35m val_fscore_macro:\u001b[94m 0.588676\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 9/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m40.26s \u001b[35mloss:\u001b[94m 0.205980\u001b[35m acc:\u001b[94m 94.020202\u001b[35m fscore_macro:\u001b[94m 0.899782\u001b[35m val_loss:\u001b[94m 1.264757\u001b[35m val_acc:\u001b[94m 66.854991\u001b[35m val_fscore_macro:\u001b[94m 0.561561\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m10/10 \u001b[35mTrain steps: \u001b[36m155 \u001b[35mVal steps: \u001b[36m34 \u001b[32m44.57s \u001b[35mloss:\u001b[94m 0.145375\u001b[35m acc:\u001b[94m 96.404040\u001b[35m fscore_macro:\u001b[94m 0.931288\u001b[35m val_loss:\u001b[94m 1.391587\u001b[35m val_acc:\u001b[94m 64.595104\u001b[35m val_fscore_macro:\u001b[94m 0.534844\u001b[0m\n",
      "Restoring data from model_t2/_rnnUni/checkpoint_epoch_5.ckpt\n"
     ]
    }
   ],
   "source": [
    "directory_name = 'model_t2/_rnnUni'\n",
    "\n",
    "model = LSTMClassifier(embedding_layer, hidden_dimension, nb_classes)\n",
    "experiment = Experiment(directory_name, \n",
    "                                model, \n",
    "                                optimizer = \"Adam\", \n",
    "                                task=\"classification\")\n",
    "        \n",
    "logging = experiment.train(train_dataloader, valid_dataloader, epochs=10, disable_tensorboard=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best checkpoint at epoch: 5\n",
      "lr: 0.001, loss: 0.607008, acc: 78.7879, fscore_macro: 0.631114, val_loss: 1.00264, val_acc: 67.42, val_fscore_macro: 0.587087\n",
      "Loading checkpoint model_t2/_rnnUni/checkpoint_epoch_5.ckpt\n",
      "Running test\n",
      "\u001b[35mTest steps: \u001b[36m34 \u001b[32m11.71s \u001b[35mtest_loss:\u001b[94m 1.002641\u001b[35m test_acc:\u001b[94m 67.419962\u001b[35m test_fscore_macro:\u001b[94m 0.587087\u001b[0m          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 11.710461083000155,\n",
       " 'test_loss': 1.002640670414474,\n",
       " 'test_acc': 67.41996236395252,\n",
       " 'test_fscore_macro': 0.5870870351791382}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des resultats ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une exactitude de 67.42% indique que le modèle est capable de prédire correctement environ deux tiers des cas de test, ce qui est une performance respectable, mais qui pourrait être améliorée pour certaines applications exigeant une précision plus élevée."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
